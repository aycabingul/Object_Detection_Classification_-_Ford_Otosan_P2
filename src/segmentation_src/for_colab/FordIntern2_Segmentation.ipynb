{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FordIntern2_Segmentation.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "rMJ7wxcu162G"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hOnLXwMhdiCF"
      },
      "source": [
        "# File Operations\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SwS17Rsics1x"
      },
      "source": [
        "!nvidia-smi "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NsXiBOsZdqOr"
      },
      "source": [
        "!cp -r /content/drive/MyDrive/InternP2/data.zip . #these operations copy data.zip into colab\n",
        "!unzip -q data.zip #unzip\n",
        "!rm data.zip #deleting the zip\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eGU_50Jfez5h"
      },
      "source": [
        "# Json2Mask-Line"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ISquYMexeBkF"
      },
      "source": [
        "#!/usr/bin/env python3\n",
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Created on Sat Jun 26 00:44:19 2021\n",
        "\n",
        "@author: aycaburcu\n",
        "\"\"\"\n",
        "import os\n",
        "import cv2\n",
        "import tqdm\n",
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "from constant import *\n",
        "from tqdm import tqdm_notebook\n",
        "\n",
        "jsons=os.listdir(JSON_DIR)#List created with names of json files in ann folder\n",
        "\n",
        "\n",
        "\n",
        "for json_name in tqdm_notebook(jsons):#access the elements in the json list\n",
        "    json_path = os.path.join(JSON_DIR, json_name)#Merged json_dir with json_name and created file path\n",
        "    json_file = open(json_path, 'r')#file reading process\n",
        "    json_dict=json.load(json_file)#Contents of json file converted to dict data type\n",
        "    mask=np.zeros((json_dict[\"size\"][\"height\"],json_dict[\"size\"][\"width\"]), dtype=np.uint8)\n",
        "   \n",
        "    \n",
        "    \n",
        "    mask_path = os.path.join(MASK_LINE_DIR, json_name[:-5])\n",
        "    # The values of the object keys in the dicts that we obtained from each \tjson file have been added to the list.\n",
        "    \n",
        "    for obj in json_dict[\"objects\"]:# To access each list inside the json_objs list\n",
        "                         \n",
        "        if obj['classTitle']=='Solid Line':\n",
        "           cv2.polylines(mask,np.array([obj['points']['exterior']],dtype=np.int32),False,color=1,thickness=14)\n",
        " \n",
        "        elif obj['classTitle']=='Dashed Line':       \n",
        "               cv2.polylines(mask,np.array([obj['points']['exterior']],dtype=np.int32),False,color=2,thickness=9)\n",
        "    \n",
        "    cv2.imwrite(mask_path,mask.astype(np.uint8))#Print filled masks in mask_path with imwrite"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WFMm8CqcVSXx"
      },
      "source": [
        "# Json2Mask-Polygon\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wp_sCOFjTi8N"
      },
      "source": [
        "\"\"\"\n",
        "Created on Sat Jun 26 00:44:19 2021\n",
        "@author: aycaburcu\n",
        "\"\"\"\n",
        "import os\n",
        "import cv2\n",
        "import tqdm\n",
        "import json\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from constant import *\n",
        "\n",
        "jsons=os.listdir(JSON_DIR)#List created with names of json files in ann folder\n",
        "\n",
        "\n",
        "\n",
        "for json_name in tqdm.tqdm(jsons):#access the elements in the json list\n",
        "    json_path = os.path.join(JSON_DIR, json_name)#Merged json_dir with json_name and created file path\n",
        "    json_file = open(json_path, 'r')#file reading process\n",
        "    json_dict=json.load(json_file)#Contents of json file converted to dict data type\n",
        "    mask=np.zeros((json_dict[\"size\"][\"height\"],json_dict[\"size\"][\"width\"]), dtype=np.uint8)\n",
        "   \n",
        "    \n",
        "    mask_path = os.path.join(MASK_POLYGON_DIR, json_name[:-5])\n",
        "    # The values of the object keys in the dicts that we obtained from each \tjson file have been added to the list.\n",
        "    \n",
        "    for obj in json_dict[\"objects\"]:# To access each list inside the json_objs list\n",
        "        \n",
        "        \n",
        "        if obj['classTitle']=='Freespace':#Objects whose classtitle is freespace\n",
        "            \n",
        "            cv2.fillPoly(mask,np.array([obj['points']['exterior']],dtype=np.int32),color=1)\n",
        "          \n",
        "            if obj['points']['interior'] !=[]: #Checking if interior list is not empty\n",
        "                for interior in obj['points']['interior']:\n",
        "                         #Converting to int32 because there are floats between points\n",
        "                         cv2.fillPoly(mask,np.array([interior],dtype=np.int32),color=0)\n",
        "                         \n",
        "\n",
        "    \n",
        "    cv2.imwrite(mask_path,mask.astype(np.uint8))#Print filled masks in mask_path with imwrite"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UKee0igLhv13"
      },
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1RDXmncAhxbi"
      },
      "source": [
        "#from line_Unet import LINE_NET\n",
        "#from polygon_model import POL_NET\n",
        "from line_SegNet import SegNet\n",
        "from preprocessing import tensorize_image, tensorize_mask, image_mask_check\n",
        "import os\n",
        "import glob\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from constant import *\n",
        "import tqdm\n",
        "import torch\n",
        "from matplotlib import pyplot as plt\n",
        "import matplotlib.ticker as mticker\n",
        "from tqdm import tqdm_notebook\n",
        "\n",
        "def train(valid_size,test_size,batch_size,epochs,cuda,input_shape,n_classes,mask_dir,model,model_save,train_if):\n",
        "    global test_input_path_list\n",
        "    ######### DIRECTORIES #########\n",
        "    SRC_DIR = os.getcwd()\n",
        "    \n",
        "    ###############################\n",
        "    \n",
        "    \n",
        "    # PREPARE IMAGE AND MASK LISTS\n",
        "    image_path_list = glob.glob(os.path.join(IMG_DIR, '*'))\n",
        "    image_path_list.sort()\n",
        "    \n",
        "    mask_path_list = glob.glob(os.path.join(MASK_DIR, '*'))\n",
        "    mask_path_list.sort()\n",
        "    \n",
        "    # DATA CHECK\n",
        "    image_mask_check(image_path_list, mask_path_list)\n",
        "    #Checked whether the elements in mask_path_list and image_path_list list are the same.\n",
        "    \n",
        "    \n",
        "    \n",
        "    # SHUFFLE INDICES\n",
        "    indices = np.random.permutation(len(image_path_list))\n",
        "    #A random array of permutations for the length of the image_path_list steps_per_epoch = len (train_input_path_list) // batch_size is created\n",
        "    \n",
        "    \n",
        "    # DEFINE TEST AND VALID INDICES\n",
        "    test_ind  = int(len(indices) * test_size)#Multiply indices length by test_size and assign it to an int-shaped variable\n",
        "    valid_ind = int(test_ind + len(indices) * valid_size)\n",
        "    \n",
        "    # SLICE TEST DATASET FROM THE WHOLE DATASET\n",
        "    test_input_path_list = image_path_list[:test_ind] #Get 0 to 476 elements of the image_path_list list\n",
        "    test_label_path_list = mask_path_list[:test_ind]#Get 0 to 476 elements of the mask_path_list list\n",
        "    \n",
        "    # SLICE VALID DATASET FROM THE WHOLE DATASET\n",
        "    valid_input_path_list = image_path_list[test_ind:valid_ind]#Get 476 to 1905 elements of the image_path_list list\n",
        "    valid_label_path_list = mask_path_list[test_ind:valid_ind]#Get 476 to 1905 elements of the mask_path_list list\n",
        "    \n",
        "    # SLICE TRAIN DATASET FROM THE WHOLE DATASET\n",
        "    train_input_path_list = image_path_list[valid_ind:]#Get the elements of the image_path_list list from 1905 to the last element\n",
        "    train_label_path_list = mask_path_list[valid_ind:]#Get the elements of the mask_path_list list from 1905 to the last element\n",
        "    #burada yukarıda vermiş olduğumuz test verisi için tüm datanın 0.1 ve validation verisi tüm datanın 0.3 içermeli\n",
        "    #Here, for the test data we have given above, all the data should contain 0.1 and all the validation data should contain 0.3, \n",
        "    #but both of them do not belong to the same data data.\n",
        "    \n",
        "    # train_input_path_list.extend(aug_path_list)\n",
        "    # train_label_path_list.extend(aug_mask_path_list)\n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "    if train_if:    \n",
        "        steps_per_epoch = len(train_input_path_list)//batch_size\n",
        "        # Find how many times to do it by dividing the length of the train data (training data) by batch_size\n",
        "        #in an epoch, a data string in the dataset goes to the end in neural networks\n",
        "        #It then waits there until the batch reaches you, the error rate is calculated after the data reaches the end\n",
        "        #Divide the training data set by 4 since our batch_size is 4\n",
        "        \n",
        "        # CALL MODEL\n",
        "        model = model\n",
        "        #Enter parameters into model and assign output to variable\n",
        "        \n",
        "        # DEFINE LOSS FUNCTION AND OPTIMIZER\n",
        "        \n",
        "    \n",
        "        criterion = nn.CrossEntropyLoss()#Creates a criterion that measures the Binary Cross Entropy between target and output:\n",
        "        #BCELoss is an acronym for Binary CrossEntropyLoss, a special case of BCOMoss CrossEntropyLoss used only for two categories of problems.\n",
        "        optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "        #Commonly used momentum beta coefficient is 0.9.\n",
        "        #lr=learning rate\n",
        "        \n",
        "        # IF CUDA IS USED, IMPORT THE MODEL INTO CUDA\n",
        "        if cuda:\n",
        "            model = model.cuda()\n",
        "        \n",
        "        val_losses=[]\n",
        "        train_losses=[]\n",
        "        # TRAINING THE NEURAL NETWORK\n",
        "        for epoch in tqdm_notebook(range(epochs)):\n",
        "        \n",
        "            running_loss = 0\n",
        "            #In each epoch, images and masks are mixed randomly in order not to output images sequentially.\n",
        "            pair_IM=list(zip(train_input_path_list,train_label_path_list))\n",
        "            np.random.shuffle(pair_IM)\n",
        "            unzipped_object=zip(*pair_IM)\n",
        "            zipped_list=list(unzipped_object)\n",
        "            train_input_path_list=list(zipped_list[0])\n",
        "            train_label_path_list=list(zipped_list[1])\n",
        "            \n",
        "            for ind in tqdm_notebook(range(steps_per_epoch)):\n",
        "                batch_input_path_list = train_input_path_list[batch_size*ind:batch_size*(ind+1)]\n",
        "                #train_input_path_list [0: 4] gets first 4 elements on first entry\n",
        "                #in the second loop train_input_list [4: 8] gets the second 4 elements\n",
        "                #element advances each time until batch_size\n",
        "                batch_label_path_list = train_label_path_list[batch_size*ind:batch_size*(ind+1)]\n",
        "                batch_input = tensorize_image(batch_input_path_list, input_shape, cuda)\n",
        "                batch_label = tensorize_mask(batch_label_path_list, input_shape, n_classes, cuda)\n",
        "                #Our data that we will insert into the model in the preprocess section is prepared by entering the parameters.\n",
        "                \n",
        "                optimizer.zero_grad()#gresets the radian otherwise accumulation occurs on each iteration\n",
        "                # Manually reset gradients after updating Weights\n",
        "                \n",
        "                \n",
        "                outputs = model(batch_input) # Give the model batch_input as a parameter and assign the resulting output to the variable.\n",
        "                \n",
        "    \n",
        "                # Forward passes the input data\n",
        "                batch_label= torch.argmax(batch_label, dim=1)\n",
        "                loss = criterion(outputs, batch_label)\n",
        "                loss.backward()# Calculates the gradient, how much each parameter needs to be updated\n",
        "                optimizer.step()# Updates each parameter according to the gradient\n",
        "        \n",
        "                running_loss += loss.item()# loss.item () takes the scalar value held in loss.\n",
        "        \n",
        "               \n",
        "                #validation \n",
        "                if ind == steps_per_epoch-1:\n",
        "                    \n",
        "                    train_losses.append(running_loss)\n",
        "                    print('training loss on epoch {}: {}'.format(epoch, running_loss))\n",
        "                    val_loss = 0\n",
        "                    for (valid_input_path, valid_label_path) in zip(valid_input_path_list, valid_label_path_list):\n",
        "                        batch_input = tensorize_image([valid_input_path], input_shape, cuda)\n",
        "                        batch_label = tensorize_mask([valid_label_path], input_shape, n_classes, cuda)\n",
        "                        batch_label= torch.argmax(batch_label, dim=1)\n",
        "                        outputs = model(batch_input)\n",
        "                        loss = criterion(outputs, batch_label)\n",
        "                        val_loss += loss.item()\n",
        "                        val_losses.append(val_loss)\n",
        "                        break\n",
        "        \n",
        "                    print('validation loss on epoch {}: {}'.format(epoch, val_loss))\n",
        "                    \n",
        "        torch.save(model,model_save)\n",
        "        print(\"Model Saved!\")\n",
        "        \n",
        "        \n",
        "        def draw_graph(val_losses,train_losses,epochs):\n",
        "            norm_validation = [float(i)/sum(val_losses) for i in val_losses]\n",
        "            norm_train = [float(i)/sum(train_losses) for i in train_losses]\n",
        "            epoch_numbers=list(range(1,epochs+1,1))\n",
        "            plt.figure(figsize=(12,6))\n",
        "            plt.subplot(2, 2, 1)\n",
        "            plt.plot(epoch_numbers,norm_validation,color=\"red\") \n",
        "            plt.gca().xaxis.set_major_locator(mticker.MultipleLocator(1))\n",
        "            plt.title('Train losses')\n",
        "            plt.subplot(2, 2, 2)\n",
        "            plt.plot(epoch_numbers,norm_train,color=\"blue\")\n",
        "            plt.gca().xaxis.set_major_locator(mticker.MultipleLocator(1))\n",
        "            plt.title('Validation losses')\n",
        "            plt.subplot(2, 1, 2)\n",
        "            plt.plot(epoch_numbers,norm_validation, 'r-',color=\"red\")\n",
        "            plt.plot(epoch_numbers,norm_train, 'r-',color=\"blue\")\n",
        "            plt.legend(['w=1','w=2'])\n",
        "            plt.title('Train and Validation Losses')\n",
        "            plt.gca().xaxis.set_major_locator(mticker.MultipleLocator(1))\n",
        "            \n",
        "            \n",
        "            plt.show()\n",
        "        \n",
        "        draw_graph(val_losses,train_losses,epochs)\n",
        "    \n",
        "#########POLYGON###############\n",
        "######### PARAMETERS ##########\n",
        "valid_size = 0.3\n",
        "test_size  = 0.1\n",
        "batch_size = 4\n",
        "epochs = 20\n",
        "cuda = True\n",
        "input_shape = (224, 224)\n",
        "n_classes = 4\n",
        "MASK_DIR=MASK_POLYGON_DIR\n",
        "model=FoInternNet(input_shape,n_classes)\n",
        "model_save='models/best_polygon_model.pt'\n",
        "train_if=True\n",
        "# ############################### \n",
        "# #polygon_model\n",
        "train(valid_size,test_size,batch_size,epochs,cuda,input_shape,n_classes,MASK_DIR,model,model_save)\n",
        "\n",
        "\n",
        "\n",
        "#########LINE###############\n",
        "######### PARAMETERS ##########\n",
        "valid_size = 0.3\n",
        "test_size  = 0.1\n",
        "batch_size = 4\n",
        "epochs = 30\n",
        "cuda = True\n",
        "input_shape = (224, 224)\n",
        "n_classes = 3\n",
        "MASK_DIR=MASK_LINE_DIR\n",
        "model=SegNet(n_classes)\n",
        "model_save='models/best_line_model.pt'\n",
        "train_if=True\n",
        "############################### \n",
        "#polygon_model\n",
        "train(valid_size,test_size,batch_size,epochs,cuda,input_shape,n_classes,MASK_DIR,model,model_save,train_if)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vf4MdD6RVrg-"
      },
      "source": [
        "!zip -r best_line_model.zip models\n",
        "!cp best_line_model.zip /content/drive/MyDrive/InternP2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M8fbD8KwWPQd"
      },
      "source": [
        "#Predict"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OXMHoBA9V_Cl"
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from constant import *\n",
        "import tqdm\n",
        "import torch\n",
        "from preprocessing import tensorize_image, tensorize_mask, image_mask_check\n",
        "import cv2\n",
        "from train import *\n",
        "from PIL import Image\n",
        "\n",
        "\n",
        "\n",
        "model_freespace= torch.load('/content/drive/MyDrive/models/Unet_1.pt')\n",
        "model_line=torch.load('/content/drive/MyDrive/models/best_line_model.pt',map_location='cuda:0')\n",
        "model_freespace=model_freespace.eval()\n",
        "model_line=model_line.eval()\n",
        "input_shape=(224,224)\n",
        "cuda=True\n",
        "if cuda:\n",
        "    model_line = model_line.cuda()\n",
        "    model_freespace=model_freespace.cuda()\n",
        "\n",
        "\n",
        "for i in tqdm.tqdm(range(len(test_input_path_list))):\n",
        "    batch_test = test_input_path_list[i:i+1]\n",
        "    test_input_line = tensorize_image(batch_test, input_shape, cuda)\n",
        "    test_input_freespace=tensorize_image(batch_test, input_shape, cuda)\n",
        "    outs_freespace = model_freespace(test_input_freespace)\n",
        "    outs_line = model_line(test_input_line)\n",
        "    out_freespace=torch.argmax(outs_freespace,axis=1)\n",
        "    out_line=torch.argmax(outs_line,axis=1)\n",
        "    out_freespace_cpu = out_freespace.cpu()\n",
        "    out_line_cpu=out_line.cpu()\n",
        "    outputs_list_freespace=out_freespace_cpu.detach().numpy()\n",
        "    outputs_list_line=out_line_cpu.detach().numpy()\n",
        "    mask_freespace=np.squeeze(outputs_list_freespace,axis=0)\n",
        "    mask_line=np.squeeze(outputs_list_line,axis=0)\n",
        "    mask_uint8_line = mask_line.astype('uint8')\n",
        "    mask_uint8_freespace = mask_freespace.astype('uint8')\n",
        "    mask_line= cv2.resize(mask_uint8_line, (1920, 1208),interpolation=cv2.INTER_NEAREST)\n",
        "    mask_freespace= cv2.resize(mask_uint8_freespace, (1920, 1208),interpolation=cv2.cv2.INTER_CUBIC)\n",
        "        \n",
        "    img=cv2.imread(batch_test[0])\n",
        "    img=cv2.resize(img,(1920, 1208))\n",
        "    mask_ind   = mask_line == 1\n",
        "    mask_ind   = mask_freespace == 1\n",
        "    cpy_img  = img.copy()\n",
        "    \n",
        "    img[mask_freespace==1,:] = (255, 0, 125)\n",
        "\n",
        "\n",
        "    img[mask_line==1,:]=(0, 0, 255)\n",
        "    img[mask_line==2,:]=(38, 255, 255)\n",
        "    \n",
        "    \n",
        "    opac_image=(img/2+cpy_img/2).astype(np.uint8)\n",
        "    predict_name=batch_test[0]\n",
        "    predict_path=predict_name.replace('images', 'predict')\n",
        "    cv2.imwrite(predict_path,opac_image.astype(np.uint8))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xfJhNMFFXdDi"
      },
      "source": [
        "!zip -r predict_full.zip data/predict\n",
        "!cp predict_full.zip /content/drive/MyDrive/InternP2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NI6wDWKd3T1J"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}